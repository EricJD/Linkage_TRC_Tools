{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "678a768a",
   "metadata": {},
   "source": [
    "## Introduction and objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ae57ee",
   "metadata": {},
   "source": [
    "In order to analyze the article texts in wikipedia without having to actively load data via the MediaWiki-API, the article texts from a selected category within Wikipedia are to be accessed and stored in textfiles wihin a defined folder. In order to perform text analysis tasks the article texts are to be cleaned from information irrelevant for the task (e.g. references, external links, etc.). The objectives are:\n",
    "- Create a list with all articles within a defined subcategory in Wikpeida\n",
    "- Access all articles within a selected subcategory via the MediaWiki-API\n",
    "- Clean the article texts and remove irrelevant information\n",
    "- Store the article texts within a textfile within a folder\n",
    "\n",
    "\n",
    "IMPORTANT: before starting this notebook, insert your Wikipedia account logindata under \"Access article text\" (ln[11]) to access the wikipedia API:\n",
    "\n",
    "    'lgname': \"[INSERT USERNAME HERE]\", \n",
    "    'lgpassword': \"[INSERT PASSWORD HERE]\","
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8605ef6d",
   "metadata": {},
   "source": [
    "## B) Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e208de5",
   "metadata": {},
   "source": [
    "#### 1) Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93e6254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from json import dumps\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d6f81f",
   "metadata": {},
   "source": [
    "Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a81c90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = os.getcwd()\n",
    "folder_path_cleaned = 'Data\\\\cleaned'\n",
    "folder_path_cleaned_articles = 'Data\\\\cleaned\\\\articles'\n",
    "folder_path_cleaned_links = 'Data\\\\cleaned\\\\links'\n",
    "abs_folder_path_cleaned = os.path.join(home_dir, folder_path_cleaned)\n",
    "abs_folder_path_cleaned_articles = os.path.join(home_dir, folder_path_cleaned_articles)\n",
    "abs_folder_path_cleaned_links = os.path.join(home_dir, folder_path_cleaned_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbb8646",
   "metadata": {},
   "source": [
    "#### 2) Method definition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78055f7",
   "metadata": {},
   "source": [
    "Get Overvied over relevant articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ad310fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subcategories(category):\n",
    "    # Method returns a list with subcategories for a selected category\n",
    "    \n",
    "    url = f\"https://en.wikipedia.org/w/api.php?action=query&format=json&list=categorymembers&cmtitle=Category:{category}&cmlimit=500\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    \n",
    "    subcategories = []\n",
    "    \n",
    "    if 'query' in data and 'categorymembers' in data['query']:\n",
    "        for member in data['query']['categorymembers']:\n",
    "            if member['ns'] == 14:\n",
    "                subcategories.append(member['title'].replace('Category:', ''))\n",
    "    \n",
    "    return subcategories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83555ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles_by_category(category):\n",
    "    # Method returns a list of articles for a selected category\n",
    "\n",
    "    url = f\"https://en.wikipedia.org/w/api.php?action=query&format=json&list=categorymembers&cmtitle=Category:{category}&cmlimit=500\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    \n",
    "    articles = {}\n",
    "    \n",
    "    if 'query' in data and 'categorymembers' in data['query']:\n",
    "        for member in data['query']['categorymembers']:\n",
    "            if member['ns'] == 0:\n",
    "                articles[member['title']] = category\n",
    "    \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bb609e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles_by_subcategory(subcategory):\n",
    "    # Method retuns a list of articles for a selected subcategory\n",
    "        \n",
    "    url = f\"https://en.wikipedia.org/w/api.php?action=query&format=json&list=categorymembers&cmtitle=Category:{subcategory}&cmlimit=500\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    \n",
    "    articles = {}\n",
    "    \n",
    "    if 'query' in data and 'categorymembers' in data['query']:\n",
    "        for member in data['query']['categorymembers']:\n",
    "            if member['ns'] == 0:\n",
    "                articles[member['title']] = subcategory\n",
    "    \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d019678",
   "metadata": {},
   "source": [
    "Specify method for purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "075c985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys(dictionary):\n",
    "    # Method returns the keys within a dictionary\n",
    "    \n",
    "    return list(dictionary.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07dd6aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_article_list(category):\n",
    "    # Method creates a list with unique titles for article concerning defined category\n",
    "    \n",
    "    article_dict = {}\n",
    "    article_dict.update(get_articles_by_category(category))\n",
    "    subcat_list = get_subcategories(category)\n",
    "    \n",
    "    for subcat in subcat_list:\n",
    "        article_dict.update(get_articles_by_subcategory(subcat))\n",
    "    \n",
    "    article_list = []\n",
    "    article_list = get_keys(article_dict)\n",
    "    \n",
    "    return article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "486b474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_article_list_with_given_selection(category, subcategory_list):\n",
    "    # Method creates a list with unique titles for article concerning defined category and subcategory\n",
    "    \n",
    "    article_dict = {}\n",
    "    \n",
    "    if category != \"\":\n",
    "        article_dict.update(get_articles_by_category(category))\n",
    "    \n",
    "    for subcat in subcategory_list:\n",
    "        article_dict.update(get_articles_by_subcategory(subcat))\n",
    "    \n",
    "    article_list = []\n",
    "    article_list = get_keys(article_dict)\n",
    "    \n",
    "    return article_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83877e8a",
   "metadata": {},
   "source": [
    "Cleaning the article texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "586dd77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_references(string):\n",
    "    # Method that removes irrelevant information from the text\n",
    " \n",
    "    # Define patterns to be removed within string\n",
    "    #pattern_one = r\"<ref name.*?/>\"\n",
    "    #pattern_two = r\"<ref>.*?</ref>\"\n",
    "    #attern_two = r\"<ref.*?>\"\n",
    "    #pattern_three = r\"<ref name=.*?</ref>\"\n",
    "    #attern_four = r\"[[File:.*?.]]\"\n",
    "    #attern_five = r\"[[Category:.*?]]\"\n",
    "    #attern_six = r\"\\\\.?\\*?\"\n",
    "    #attern_seven = r\"Help:List.*?\"\n",
    "    #attern_eight = r\"wikt:.*?\"\n",
    "    #attern_nine = r\"{.*?}}?\"\n",
    "    #attern_ten = r\"'''.*?\"\n",
    "    \n",
    "    # Order and application of patterns\n",
    "    #ed_string = re.sub(pattern_ten, \"\", \n",
    "    #                   re.sub(pattern_nine, \"\", \n",
    "    #                          re.sub(pattern_eight, \"\", \n",
    "    #                                 re.sub(pattern_seven, \"\", \n",
    "    #                                       re.sub(pattern_six, \"\", \n",
    "    #                                              re.sub(pattern_five, \"\", \n",
    "    #                                                    re.sub(pattern_four, \"\",\n",
    "    #                                                           #re.sub(pattern_three, \"\", \n",
    "    #                                                           re.sub(pattern_two, \"\", string))))))))\n",
    "    #                                                                         #re.sub(pattern_one, \"\", string)\n",
    "    #                                                                            #))\n",
    "          \n",
    "    red_string = re.sub(r\"(<ref.*?>)|(<\\/ref>)|(\\[\\[File:.*?\\]\\])|(\\[\\[Category:.*?\\]\\])|(\\\\n?\\*?)|(Help:List.*?)|(wikt:.*?)|(\\{\\{reflist.+\\]\\})|('''.*?)|(<math>.*?<\\/math>)|({pages.*?contentmodel: wikitext, \\*: )\", \"\", string)\n",
    "    \n",
    "    # Additionally remove reference list\n",
    "    result = red_string[0:red_string.find(\"{{Refbegin}}\")]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d76e714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_special_characters(input_string):\n",
    "    # Method that cleans/replaces undesired combinations of characters within the string\n",
    "    \n",
    "    # Define the characters and strings to be replaced\n",
    "    formatted_string = input_string.replace(\"}nn\", \"\")\n",
    "    formatted_string = formatted_string.replace(\"}n\", \"\")  \n",
    "    formatted_string = formatted_string.replace(\"[[\", \"\") \n",
    "    formatted_string = formatted_string.replace(\"]]\", \"\") \n",
    "    formatted_string = formatted_string.replace(\"==n\", \"== \")\n",
    "    formatted_string = formatted_string.replace(\"nn==\", \" ==\")\n",
    "    formatted_string = formatted_string.replace(\"n==\", \" ==\")\n",
    "    formatted_string = formatted_string.replace(\"}</ref>\", \" \")\n",
    "    formatted_string = input_string.replace('\"\\n', '') # relevant to avoid problem with \"Twiddler's syndrome\"\n",
    "    formatted_string = formatted_string.replace(\"\\\\\\\\\", \"\")\n",
    "    formatted_string = formatted_string.replace(\"u2013\", \"-\")\n",
    "    formatted_string = formatted_string.replace(\"u00f6\", \"ö\")\n",
    "    formatted_string = formatted_string.replace(\"00fc\", \"ü\")  \n",
    "    formatted_string = formatted_string.replace(\"u00e4\", \"ä\")  \n",
    "    formatted_string = formatted_string.replace(\"u00fc\", \"ü\")\n",
    "    formatted_string = formatted_string.replace(\"u00e9\", \"é\")\n",
    "    formatted_string = formatted_string.replace(\"u00e2\", \"â\")\n",
    "    formatted_string = formatted_string.replace(\"u00e8\", \"è\")\n",
    "    formatted_string = formatted_string.replace(\"u00e0\", \"à\")\n",
    "    formatted_string = formatted_string.replace(\"u00f3\", \"ó\")\n",
    "    formatted_string = formatted_string.replace(\"u00e1\", \"á\")\n",
    "    formatted_string = formatted_string.replace(\"u00f8\", \"ø\")\n",
    "    formatted_string = formatted_string.replace('\"', '')\n",
    "    \n",
    "\n",
    "    return formatted_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ab321e",
   "metadata": {},
   "source": [
    "Access article text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a460dbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_text(title):\n",
    "    # Method accesses the MediaWiki API and queries the defined article via the title\n",
    "    \n",
    "    # Access MediaWiki API\n",
    "    session = requests.Session()\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    \n",
    "    params_0 = {\n",
    "    'action':\"query\",\n",
    "    'meta':\"tokens\",\n",
    "    'type':\"login\",\n",
    "    'format':\"json\"\n",
    "    }\n",
    "    \n",
    "    req = session.get(url=url, params= params_0)\n",
    "    data = req.json()\n",
    "    \n",
    "    login_token = data['query']['tokens']['logintoken']\n",
    "    \n",
    "    params_1 = {\n",
    "    'action': \"login\",\n",
    "    'lgname': \"[INSERT USERNAME HERE]\", \n",
    "    'lgpassword': \"[INSERT PASSWORD HERE]\",\n",
    "    'lgtoken': login_token,\n",
    "    'format': \"json\"\n",
    "    }\n",
    "    req = session.post(url, data=params_1)\n",
    "    data = req.json()\n",
    "    \n",
    "    # Get article text\n",
    "    content_params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"titles\": title,\n",
    "        \"rvprop\": \"content\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    \n",
    "    content_response = session.get(url, params=content_params).json()\n",
    "\n",
    "    return content_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fd55bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tostring(dictionary):\n",
    "    # Method converts type dict to type string\n",
    "    \n",
    "    reduced_dictionary = dictionary['query']\n",
    "    convert = dumps(reduced_dictionary)\n",
    "    return convert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896b5323",
   "metadata": {},
   "source": [
    "Store extracted article text as text files within defined folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d2f977c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_and_save_data(links_list, abs_folder_path_cleaned):\n",
    "    for link in links_list:\n",
    "        content = get_article_text(link)\n",
    "        content_str = convert_tostring(content)\n",
    "        content_str_without_ref = remove_references(content_str)\n",
    "        content_str_clean =  clean_special_characters(content_str_without_ref)    \n",
    "    \n",
    "        data_name = link\n",
    "        \n",
    "        path = abs_folder_path_cleaned\n",
    "        \n",
    "        # Save article text with name\n",
    "        try:\n",
    "            data_path = path + '/' + data_name + \".txt\"\n",
    "            with open(data_path, 'w') as data:\n",
    "                data.write(content_str_clean)\n",
    "\n",
    "        except IOError:\n",
    "            print(\"Fehler beim Schreiben der Datei: \" + data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f2a23ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_article_text(file_name, folder_path):\n",
    "    # Method extracts article text from text file and returns a string\n",
    "    \n",
    "    # Define file path incl. ending .txt\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    file_path += \".txt\"\n",
    "\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "        return content\n",
    "    else:\n",
    "        \n",
    "        # If the article is not wihin the folder - return an empty string\n",
    "        print(f\"Die Datei '{file_name}' wurde nicht im Ordner '{folder_path}' gefunden.\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5851c20",
   "metadata": {},
   "source": [
    "Extract links from article text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e091ad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links(string):\n",
    "    # Method extracts links from article text and returns a list of strings\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    # Links within Wikipedia articles are marked by '[[...]]'\n",
    "    start = string.find(\"[[\")\n",
    "    while start != -1:\n",
    "        end = string.find(\"]]\", start + 1)\n",
    "        if end == -1:\n",
    "            break\n",
    "        \n",
    "        # Check if it is a masked link\n",
    "        linked_content = string[start + 2:end]\n",
    "        \n",
    "        \n",
    "        # If so remove first part (up to |)\n",
    "        if '|' in linked_content:\n",
    "            linked_content = linked_content[:linked_content.find(\"|\")]\n",
    "        \n",
    "        result.append(linked_content)\n",
    "        start = string.find(\"[[\", end + 1)        \n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb40e6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_entries(entry_list):\n",
    "    # Method that removes empty entries from a list and returns a list\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            entry_list.remove(\"\")\n",
    "            \n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    return entry_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cae74e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_multiple_entries(entry_list):\n",
    "    # Method meant to clean multiple entries within the nodes - a node is unique within the linkage graph\n",
    "    \n",
    "    # Transform list into a set and then back into a list\n",
    "    unique_list = list(set(entry_list))\n",
    "    \n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e9166ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_variables(string_list):\n",
    "    # Method that removes variables which are not unique within list and returns a list of unique entris\n",
    "    # -> A link declaration could be \"Disease\" or \"disease\" by deriving the same neo4j variable only a single node\n",
    "    # is created within the linkage graph\n",
    "    \n",
    "    string_dict = {}\n",
    "    \n",
    "    # Create dict entry with neo4j variable as key and extracted link as value\n",
    "    for string in string_list:\n",
    "        key = convert_to_neo4j_variable(string)\n",
    "        value = string\n",
    "        \n",
    "        # Check if key exists already in dict\n",
    "        if key not in string_dict:\n",
    "            string_dict[key] = value\n",
    "    \n",
    "    unique_values = list(string_dict.values())\n",
    "    \n",
    "    return unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f78dde6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_neo4j_variable(string):\n",
    "    # Method that convers extracted link to neo4j variable\n",
    "    \n",
    "    # Replace all characters not admitted for variable name\n",
    "    cleaned_string = re.sub(r'[^a-zA-Z0-9_]', '_', string)\n",
    "    \n",
    "    # If first letter is a digit, add \"_\"\n",
    "    if cleaned_string[0].isdigit():\n",
    "        cleaned_string = '_' + cleaned_string\n",
    "    \n",
    "    # Change capital letters wihin variable name\n",
    "    cleaned_string = cleaned_string.lower() \n",
    "    \n",
    "    return cleaned_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a26a2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nodes_from_article_list(articles_list):\n",
    "    # Method that extracts all nodes from all articles within the defined article list and folder\n",
    "    \n",
    "    nodes = []\n",
    "    \n",
    "    # Iterate over article list\n",
    "    for article in articles_list:\n",
    "        \n",
    "        # Add the original article titel -> cleaned\n",
    "        nodes.append(clean_special_characters(article))\n",
    "        \n",
    "        # Read article text from txt file in folder\n",
    "        article_text = read_article_text(article, abs_folder_path_cleaned_articles)\n",
    "\n",
    "        # Remove references\n",
    "        text_without_references = remove_references(article_text)\n",
    "\n",
    "        # Clean special characters in string\n",
    "        text_cleaned = clean_special_characters(text_without_references)\n",
    "\n",
    "        # Extract links from text\n",
    "        text_links = extract_links(text_cleaned)\n",
    "\n",
    "        # add extracted links zu nodes list\n",
    "        nodes.extend(text_links)\n",
    "    \n",
    "    # clean multiple entries -> each node is unique\n",
    "    nodes_cleaned = clean_multiple_entries(nodes)\n",
    "    \n",
    "    # Remove empty entries within list\n",
    "    nodes_cleaned_without_empty_entries = remove_empty_entries(nodes_cleaned)\n",
    "    \n",
    "    # Remove duplicate variable declarations\n",
    "    nodes_cleaned_no_duplicates = remove_duplicate_variables(nodes_cleaned_without_empty_entries)\n",
    "    \n",
    "    return nodes_cleaned_no_duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02582de9",
   "metadata": {},
   "source": [
    "#### 3) Main-Method - Extraction of article titles and article texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97a8fce",
   "metadata": {},
   "source": [
    "This variable is critical: Enter the name(s) of the categories to download here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6d325f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of articles within subcategory \n",
    "subcat_list_cat = ['Radioactive contamination', 'Pollutants']\n",
    "article_list = create_article_list_with_given_selection(\"\", subcat_list_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89f9dc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save list with article titles as csv-file\n",
    "with open(abs_folder_path_cleaned + '//' + \"list\" + '/' + \"Article_list_Pollution_n+1\" + \".txt\", 'w', newline='') as myfile:\n",
    "     wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "     wr.writerow(article_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a515af74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract article texts according to list of articles, clean text and store it\n",
    "extract_text_and_save_data(article_list, abs_folder_path_cleaned_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfb3a0e",
   "metadata": {},
   "source": [
    "c) Extract all links within the article texts of articles from 'article_list_pollution.txt' and save it as list 'article_list_pollution_n+1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62a8d183",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_list = extract_nodes_from_article_list(article_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d34f05e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3911"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "936f61e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save list with article titles as text file \n",
    "with open(abs_folder_path_cleaned + '//' + \"list\" + '/' + \"Article_list_Pollution_n+1_links\" + \".txt\", 'w', newline='') as myfile:\n",
    "     wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "     wr.writerow(nodes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a90a264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fehler beim Schreiben der Datei: :fr :Association pour le contru00f4le de la radioactivité dans l'Ouest\n",
      "Fehler beim Schreiben der Datei: creativecommons:by/4.0/\n",
      "Fehler beim Schreiben der Datei: :File:A RES 71 313 E.pdf\n",
      "Fehler beim Schreiben der Datei: :File:N1529189.pdf\n",
      "Fehler beim Schreiben der Datei: Bofors 40 mm Automatic Gun L/60\n",
      "Fehler beim Schreiben der Datei: HIV/AIDS in the United States\n",
      "Fehler beim Schreiben der Datei: : dilute\n",
      "Fehler beim Schreiben der Datei: :clumping\n",
      "Fehler beim Schreiben der Datei: MARPOL 73/78\n",
      "Fehler beim Schreiben der Datei: :dilute\n",
      "Fehler beim Schreiben der Datei: R/V\n",
      "Fehler beim Schreiben der Datei: R/V Ocean Starr\n",
      "Fehler beim Schreiben der Datei: MARPOL 73/78#Annexes\n"
     ]
    }
   ],
   "source": [
    "# Extract article texts according to list of articles \n",
    "extract_text_and_save_data(nodes_list, abs_folder_path_cleaned_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc5aecb",
   "metadata": {},
   "source": [
    "e) If there are exceptions due to the large number of articles - restart the the method at the point it failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b798a508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fehler beim Schreiben der Datei: :fr :Association pour le contru00f4le de la radioactivité dans l'Ouest\n",
      "Fehler beim Schreiben der Datei: ISO/IEC 17025\n"
     ]
    }
   ],
   "source": [
    "# Nach Fehler weitermachen - evtl. Exception mit einfügen\n",
    "extract_text_and_save_data(nodes_list[12000:], abs_folder_path_cleaned_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce08f54c",
   "metadata": {},
   "source": [
    "####  4) Testing and post preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17011d4",
   "metadata": {},
   "source": [
    "The articles \"HIV/AIDS\" and \"Opitz G/BBB syndrome\" have been downloaded and stored as text files with the name \"HIVAIDS\" and \"OPITZ GBBB syndrome\" in the defined folder. A post processing of the article's text is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae29e9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_article_text(file_name, folder_path):\n",
    "    \n",
    "    # Define File path incl. ending .txt\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    file_path += \".txt\"\n",
    "\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "        return content\n",
    "    else:\n",
    "        \n",
    "        # If the article is not wihin the folder - return an empty string\n",
    "        print(f\"Die Datei '{file_name}' wurde nicht im Ordner '{folder_path}' gefunden.\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ef0d6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the text file \"HIVAIDS\" from the folder\n",
    "hivaids_new = read_article_text(\"HIVAIDS\",abs_folder_path_cleaned )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84299959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the text file \"Opitz GBBB syndrome\" from the folder\n",
    "opitz_new = read_article_text(\"Opitz GBBB syndrome\",abs_folder_path_cleaned )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc725e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean article text for \"HIV/AIDS\" and store string within text file \"HIVAIDS\"\n",
    "hivaids_str_without_ref = remove_references(hivaids_new)\n",
    "hivaids_str_clean =  clean_special_characters(hivaids_str_without_ref)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e87af16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean article text for \"\" and store string within text file \"HIVAIDS\"\n",
    "opitz_str_without_ref = remove_references(opitz_new)\n",
    "opitz_str_clean =  clean_special_characters(opitz_str_without_ref)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "838e8f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save article text for \"HIV/AIDS\" within text file \"HIVAIDS\"\n",
    "try:\n",
    "    data_path = abs_folder_path_cleaned + '/' + \"HIVAIDS\" + \".txt\"\n",
    "    with open(data_path, 'w') as data:\n",
    "        data.write(hivaids_str_clean)\n",
    "except IOError:\n",
    "    print(\"Fehler beim Schreiben der Datei: \" + \"HIVAIDS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3309f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save article text for \"Opitz G/BBB syndrome\" within text file \"Opitz GBBB syndrome\"\n",
    "try:\n",
    "    data_path = abs_folder_path_cleaned + '/' + \"Opitz GBBB syndrome\" + \".txt\"\n",
    "    with open(data_path, 'w') as data:\n",
    "        data.write(opitz_str_clean)\n",
    "except IOError:\n",
    "    print(\"Fehler beim Schreiben der Datei: \" + \"Opitz GBBB syndrome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cc7654",
   "metadata": {},
   "source": [
    "The articles \"HIV/AIDS\" and \"Opitz G/BBB syndrome\" have been downloaded and stored as text files with the name \"HIVAIDS\" and \"OPITZ GBBB syndrome\" in the defined folder. For the Creation of the linkage graph, two exceptions will be implemented for these articles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
